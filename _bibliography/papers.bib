---
---

@misc{doumbouya2024h4rm3ldynamicbenchmarkcomposable,
      title={h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment}, 
      author={Moussa Koulako Bala Doumbouya and Ananjan Nandi and Gabriel Poesia and Davide Ghilardi and Anna Goldie and Federico Bianchi and Dan Jurafsky and Christopher D. Manning},
      year={2024},
      eprint={2408.04811},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2408.04811}, 
}

@inproceedings{ghilardi-etal-2024-accelerating,
    title = "Accelerating Sparse Autoencoder Training via Layer-Wise Transfer Learning in Large Language Models",
    author = "Ghilardi, Davide  and
      Belotti, Federico  and
      Molinari, Marco  and
      Lim, Jaehyuk",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.32",
    pages = "530--550",
    abstract = "Sparse AutoEncoders (SAEs) have gained popularity as a tool for enhancing the interpretability of Large Language Models (LLMs). However, training SAEs can be computationally intensive, especially as model complexity grows. In this study, the potential of transfer learning to accelerate SAEs training is explored by capitalizing on the shared representations found across adjacent layers of LLMs. Our experimental results demonstrate that fine-tuning SAEs using pre-trained models from nearby layers not only maintains but often improves the quality of learned representations, while significantly accelerating convergence. These findings indicate that the strategic reuse of pretrained SAEs is a promising approach, particularly in settings where computational resources are constrained.",
}

@misc{ghilardi2024efficienttrainingsparseautoencoders,
      title={Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups}, 
      author={Davide Ghilardi and Federico Belotti and Marco Molinari},
      year={2024},
      eprint={2410.21508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21508}, 
}